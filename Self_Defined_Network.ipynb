{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self Defined Network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFlxsJjAW2sC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRJ395qRXE8k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b026a983-2c66-4cf2-ddba-16ade3650eff"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnOrJiEuXn61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = '/content/drive/My Drive/flower_photos/'\n",
        "train_dir = os.path.join(data_dir, 'train/')\n",
        "test_dir = os.path.join(data_dir, 'test/')\n",
        "\n",
        "classes = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSbd760gYoBG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a4dc6c0b-16a0-4def-e8d5-5b54396cc66b"
      },
      "source": [
        "transform = torchvision.transforms.Compose([torchvision.transforms.RandomResizedCrop(224),\n",
        "                                            torchvision.transforms.RandomHorizontalFlip(p = 0.5),\n",
        "                                            torchvision.transforms.RandomVerticalFlip(p = 0.2),\n",
        "                                            torchvision.transforms.RandomRotation((-45, 45)),\n",
        "                                            torchvision.transforms.ToTensor()])\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder(train_dir, transform = transform)\n",
        "test_data = torchvision.datasets.ImageFolder(test_dir, transform = transform)\n",
        "\n",
        "print (len(train_data))\n",
        "print (len(test_data))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3139\n",
            "540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JRAcsdQYqL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0283e31e-a416-4021-b2b1-d4bec2225865"
      },
      "source": [
        "batch_size = 20\n",
        "num_workers = 0\n",
        "indices = [i for i in range(0, len(train_data))]\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(0.8 * len(train_data)))\n",
        "print(split)\n",
        "train_idx, valid_idx = indices[:split], indices[split:]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLad6HFUYsXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, num_workers = num_workers, sampler = train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, num_workers = num_workers, sampler = valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, num_workers = num_workers)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FxxTiuOY2kM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBNXvrwAakP_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "c7013fb2-18a2-4fbe-f4d4-18eefa66c929"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super (Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, 3, padding = 1)\n",
        "    self.conv2 = nn.Conv2d(16, 64, 3, padding = 1)\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3, padding = 1)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.fc1 = nn.Linear(128*28*28, 1000)\n",
        "    self.fc2 = nn.Linear(1000, 100)\n",
        "    self.fc3 = nn.Linear(100, 5)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = self.pool(F.relu(self.conv3(x)))\n",
        "\n",
        "    x = x.view(-1, 128*28*28)\n",
        "    x = self.dropout(x)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "model = Net()\n",
        "print(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=100352, out_features=1000, bias=True)\n",
            "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
            "  (fc3): Linear(in_features=100, out_features=5, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41cOzcUvfkLd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "f91d81eb-2467-4022-c704-eedb58a1dce4"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "n_epochs = 30\n",
        "model.cuda()\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "\n",
        "  model.train()\n",
        "  for data, target in train_loader:\n",
        "    data, target = data.cuda(), target.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()*data.size(0)\n",
        "  \n",
        "  model.eval()\n",
        "  for data, target in valid_loader:\n",
        "    data, target = data.cuda(), target.cuda()\n",
        "    output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "  train_loss = train_loss/len(train_loader.sampler)\n",
        "  valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "\n",
        "  print(\"Epoch {}, Training Loss {:.6f}, Validation Loss {:.6f}\".format(epoch, train_loss, valid_loss))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Training Loss 1.599550, Validation Loss 1.606935\n",
            "Epoch 2, Training Loss 1.595628, Validation Loss 1.603059\n",
            "Epoch 3, Training Loss 1.587179, Validation Loss 1.583716\n",
            "Epoch 4, Training Loss 1.563900, Validation Loss 1.571642\n",
            "Epoch 5, Training Loss 1.513712, Validation Loss 1.532923\n",
            "Epoch 6, Training Loss 1.435778, Validation Loss 1.352560\n",
            "Epoch 7, Training Loss 1.331724, Validation Loss 1.354478\n",
            "Epoch 8, Training Loss 1.263634, Validation Loss 1.286020\n",
            "Epoch 9, Training Loss 1.235408, Validation Loss 1.246190\n",
            "Epoch 10, Training Loss 1.227602, Validation Loss 1.205172\n",
            "Epoch 11, Training Loss 1.205229, Validation Loss 1.198198\n",
            "Epoch 12, Training Loss 1.199090, Validation Loss 1.197395\n",
            "Epoch 13, Training Loss 1.181949, Validation Loss 1.148935\n",
            "Epoch 14, Training Loss 1.167832, Validation Loss 1.138760\n",
            "Epoch 15, Training Loss 1.144218, Validation Loss 1.169638\n",
            "Epoch 16, Training Loss 1.137426, Validation Loss 1.213935\n",
            "Epoch 17, Training Loss 1.123340, Validation Loss 1.063961\n",
            "Epoch 18, Training Loss 1.115959, Validation Loss 1.091829\n",
            "Epoch 19, Training Loss 1.091559, Validation Loss 1.068220\n",
            "Epoch 20, Training Loss 1.085377, Validation Loss 1.088117\n",
            "Epoch 21, Training Loss 1.093775, Validation Loss 1.102278\n",
            "Epoch 22, Training Loss 1.076797, Validation Loss 1.081026\n",
            "Epoch 23, Training Loss 1.060494, Validation Loss 1.040787\n",
            "Epoch 24, Training Loss 1.074667, Validation Loss 1.052556\n",
            "Epoch 25, Training Loss 1.054708, Validation Loss 1.089460\n",
            "Epoch 26, Training Loss 1.057353, Validation Loss 0.979842\n",
            "Epoch 27, Training Loss 1.041572, Validation Loss 0.990112\n",
            "Epoch 28, Training Loss 1.032937, Validation Loss 1.000755\n",
            "Epoch 29, Training Loss 1.053299, Validation Loss 1.036456\n",
            "Epoch 30, Training Loss 1.036704, Validation Loss 0.972222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JDFUUdQieex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss = 0.0\n",
        "class_correct = [0 for i in range(0, 5)]\n",
        "class_total = [0 for i in range(0, 5)]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzN6uxgCi7QR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "for data, target in test_loader:\n",
        "  data, target = data.cuda(), target.cuda()\n",
        "  output = model(data)\n",
        "  loss = criterion(output, target)\n",
        "  test_loss = test_loss + loss.item()*data.size(0)\n",
        "  _, pred = torch.max(output, 1)\n",
        "  correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "  correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "\n",
        "  for i in range(0, batch_size):\n",
        "    label = target.data[i]\n",
        "    class_correct[label] += correct[i].item()\n",
        "    class_total[label] += 1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXwucoAikQR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59597295-20e1-4355-c30e-5c49b74c48ad"
      },
      "source": [
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print(test_loss)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9941896023573699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR5SIef-yWhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "570dfb3c-e237-411b-c127-5c53bbedc572"
      },
      "source": [
        "for i in range(5):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of daisy: 67% (62/92)\n",
            "Test Accuracy of dandelion: 63% (84/132)\n",
            "Test Accuracy of roses: 13% (12/91)\n",
            "Test Accuracy of sunflowers: 81% (82/101)\n",
            "Test Accuracy of tulips: 62% (78/124)\n",
            "\n",
            "Test Accuracy (Overall): 58% (318/540)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}